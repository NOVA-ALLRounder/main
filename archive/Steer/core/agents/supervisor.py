"""Supervisor LLM helper shared across agents."""
from __future__ import annotations

import json
import os
import shutil
import subprocess
import textwrap
from dataclasses import dataclass, field
from typing import Any, Dict, List, Optional

from core.utils import get_logger

LOGGER = get_logger("agents.supervisor")

DEFAULT_SUPERVISOR_PROMPT = textwrap.dedent(
    """
    You are a senior QA supervisor reviewing outputs generated by automated agents.
    Evaluate the provided summary information and decide the next action.

    Available actions:
      - "accept"   : output looks good; no further action required.
      - "review"   : request another automated pass; provide focus keywords or notes.
      - "escalate" : requires manual review by a human operator; explain briefly.

    Respond strictly with JSON:
    {
      "action": "accept" | "review" | "escalate",
      "reason": "<short justification>",
      "focus_keywords": ["keyword1", "..."],
      "notes": "<optional additional notes>"
    }

    Summary payload:
    {payload}
    """
).strip()


@dataclass
class SupervisorDecision:
    action: str
    reason: str = ""
    focus_keywords: List[str] = field(default_factory=list)
    notes: str = ""

    def as_dict(self) -> Dict[str, Any]:
        return {
            "action": self.action,
            "reason": self.reason,
            "focus_keywords": list(self.focus_keywords),
            "notes": self.notes,
        }


@dataclass
class SupervisorConfig:
    backend: str
    model: str
    host: str
    prompt: str
    timeout: float
    num_predict: int
    temperature: float

    @classmethod
    def from_env(cls, agent_prefix: str = "MEETING") -> "SupervisorConfig":
        prefix = agent_prefix.upper()
        backend = os.getenv(f"{prefix}_SUPERVISOR_BACKEND") or os.getenv("SUMMARY_SUPERVISOR_BACKEND", "")
        backend = backend.strip().lower()

        model = (
            os.getenv(f"{prefix}_SUPERVISOR_MODEL", "")
            or os.getenv("SUMMARY_SUPERVISOR_MODEL", "")
            or os.getenv(f"{prefix}_SUMMARY_REVIEW_MODEL", "")
            or os.getenv(f"{prefix}_SUMMARY_OLLAMA_MODEL", "")
            or os.getenv("LNPCHAT_LLM_MODEL", "")
        )
        model = model.strip()

        host = os.getenv(f"{prefix}_SUPERVISOR_HOST") or os.getenv("SUMMARY_SUPERVISOR_HOST", "")
        host = host.strip()

        prompt = os.getenv(f"{prefix}_SUPERVISOR_PROMPT") or os.getenv("SUMMARY_SUPERVISOR_PROMPT", "") or DEFAULT_SUPERVISOR_PROMPT
        prompt = prompt.strip()

        timeout_env = os.getenv(f"{prefix}_SUPERVISOR_TIMEOUT") or os.getenv("SUMMARY_SUPERVISOR_TIMEOUT", "")
        try:
            timeout = float(timeout_env) if timeout_env else 45.0
        except ValueError:
            LOGGER.warning("Invalid SUPERVISOR timeout=%s; falling back to 45s", timeout_env)
            timeout = 45.0

        num_predict_env = (
            os.getenv(f"{prefix}_SUPERVISOR_NUM_PREDICT")
            or os.getenv("SUMMARY_SUPERVISOR_NUM_PREDICT")
            or "160"
        )
        temperature_env = (
            os.getenv(f"{prefix}_SUPERVISOR_TEMPERATURE")
            or os.getenv("SUMMARY_SUPERVISOR_TEMPERATURE")
            or "0.05"
        )
        try:
            num_predict = max(64, int(num_predict_env))
        except ValueError:
            num_predict = 256
        try:
            temperature = float(temperature_env)
        except ValueError:
            temperature = 0.15

        return cls(
            backend=backend,
            model=model,
            host=host,
            prompt=prompt,
            timeout=max(1.0, timeout),
            num_predict=num_predict,
            temperature=max(0.0, temperature),
        )


class SummarySupervisor:
    """LLM-based supervisor that evaluates agent outputs and suggests next actions."""

    def __init__(self, config: SupervisorConfig) -> None:
        self.config = config
        self.backend = config.backend
        self._enabled = bool(self.backend)
        self._ollama_cmd = None
        self.model: Optional[str] = None

        if self.backend == "ollama":
            self._ollama_cmd = shutil.which("ollama")
            if not self._ollama_cmd:
                LOGGER.warning("ollama supervisor backend requested but executable not found; disabling supervisor")
                self._enabled = False
            else:
                self.model = self._resolve_model(self.config.model)
                if self.model:
                    LOGGER.info("supervisor configured: backend=ollama model=%s", self.model)
                else:
                    LOGGER.info("supervisor backend=ollama; model will be resolved dynamically")
        elif self.backend:
            LOGGER.warning("Unsupported supervisor backend '%s'; disabling supervisor", self.backend)
            self._enabled = False

    @classmethod
    def from_env(cls, agent: str = "MEETING") -> "SummarySupervisor":
        return cls(SupervisorConfig.from_env(agent_prefix=agent))

    def is_enabled(self) -> bool:
        return self._enabled

    def decide(
        self,
        agent: str,
        summary: Any,
        metrics: Dict[str, Any],
        issues: Optional[List[str]] = None,
        alerts: Optional[List[str]] = None,
    ) -> SupervisorDecision:
        if not self._enabled:
            return SupervisorDecision(action="accept")

        payload = self._build_payload(agent, summary, metrics, issues=issues, alerts=alerts)
        prompt = self.config.prompt.format(payload=json.dumps(payload, ensure_ascii=False, indent=2))
        raw = self._invoke_backend(prompt)
        if raw is None:
            return SupervisorDecision(action="accept")

        decision = self._parse_response(raw)
        if decision is None:
            return SupervisorDecision(action="accept")
        return decision

    # ------------------------------------------------------------------
    # Prompt helpers
    # ------------------------------------------------------------------
    def _build_payload(
        self,
        agent: str,
        summary: Any,
        metrics: Dict[str, Any],
        *,
        issues: Optional[List[str]],
        alerts: Optional[List[str]],
    ) -> Dict[str, Any]:
        structured = getattr(summary, "structured_summary", {}) or {}
        return {
            "agent": agent,
            "raw_summary": getattr(summary, "raw_summary", ""),
            "highlights": getattr(summary, "highlights", []),
            "action_items": getattr(summary, "action_items", []),
            "decisions": getattr(summary, "decisions", []),
            "issues": issues or structured.get("review_issues") or [],
            "alerts": alerts or structured.get("alerts") or [],
            "review_notes": structured.get("review_notes"),
            "generated_by": structured.get("generated_by"),
            "metrics": metrics,
        }

    # ------------------------------------------------------------------
    # Backend invocation
    # ------------------------------------------------------------------
    def _invoke_backend(self, prompt: str) -> Optional[str]:
        if self.backend == "ollama":
            return self._invoke_ollama(prompt)
        return None

    def _invoke_ollama(self, prompt: str) -> Optional[str]:
        if not self._ollama_cmd:
            return None
        model = self.model or self._resolve_model(self.config.model)
        if not model:
            LOGGER.warning("ollama supervisor has no model configured; skipping decision")
            return None
        env = os.environ.copy()
        if self.config.host:
            env["OLLAMA_HOST"] = self.config.host
        env.setdefault("OLLAMA_NUM_PREDICT", str(self.config.num_predict))
        env.setdefault("OLLAMA_TEMPERATURE", str(self.config.temperature))
        try:
            result = subprocess.run(
                ["ollama", "run", model],
                input=prompt,
                capture_output=True,
                text=True,
                env=env,
                timeout=self.config.timeout,
                check=False,
            )
        except subprocess.TimeoutExpired:
            LOGGER.warning("ollama supervisor timed out after %.1fs", self.config.timeout)
            return None
        except FileNotFoundError:
            LOGGER.warning("ollama executable missing; disabling supervisor")
            self._enabled = False
            return None

        if result.returncode != 0:
            stderr = (result.stderr or "").strip()
            stdout = (result.stdout or "").strip()
            LOGGER.warning("ollama supervisor failed (%s): %s", result.returncode, stderr or stdout or "unknown error")
            return None
        return (result.stdout or "").strip()

    # ------------------------------------------------------------------
    # Response parsing
    # ------------------------------------------------------------------
    def _parse_response(self, text: str) -> Optional[SupervisorDecision]:
        cleaned = text.strip()
        if not cleaned:
            return None
        if cleaned.startswith("```"):
            cleaned = self._strip_code_fence(cleaned)
        try:
            payload = json.loads(cleaned)
        except json.JSONDecodeError:
            return None
        if not isinstance(payload, dict):
            return None

        action = str(payload.get("action") or "").strip().lower()
        if action not in {"accept", "review", "escalate"}:
            action = "accept"
        reason = str(payload.get("reason") or "").strip()
        notes = str(payload.get("notes") or "").strip()

        focus_raw = payload.get("focus_keywords") or []
        focus_keywords: List[str] = []
        if isinstance(focus_raw, list):
            for entry in focus_raw:
                if isinstance(entry, str):
                    trimmed = entry.strip()
                    if trimmed:
                        focus_keywords.append(trimmed)

        return SupervisorDecision(action=action, reason=reason, focus_keywords=focus_keywords, notes=notes)

    @staticmethod
    def _strip_code_fence(text: str) -> str:
        parts = text.split("```")
        if len(parts) >= 2:
            return parts[1].strip()
        return text.strip()

    # ------------------------------------------------------------------
    # Model resolution helpers
    # ------------------------------------------------------------------
    def _resolve_model(self, raw: str) -> Optional[str]:
        candidates = self._candidate_models(raw)
        if self.backend == "ollama" and self._ollama_cmd:
            available = self._list_ollama_models()
            if available:
                for candidate in candidates:
                    if candidate in available:
                        return candidate
        for candidate in candidates:
            if candidate:
                return candidate
        return None

    def _candidate_models(self, raw: str) -> List[str]:
        candidates: List[str] = []
        if raw:
            candidates.extend([item.strip() for item in raw.replace(";", ",").split(",") if item.strip()])
        fallbacks = [
            "eeve_korean_v2",
            os.getenv("SUMMARY_SUPERVISOR_MODEL", ""),
            os.getenv("MEETING_SUMMARY_REVIEW_MODEL", ""),
            os.getenv("MEETING_SUMMARY_OLLAMA_MODEL", ""),
            os.getenv("LNPCHAT_LLM_MODEL", ""),
            "llama3",
        ]
        for item in fallbacks:
            if not item:
                continue
            for token in item.replace(";", ",").split(","):
                trimmed = token.strip()
                if trimmed and trimmed not in candidates:
                    candidates.append(trimmed)
        return candidates

    def _list_ollama_models(self) -> List[str]:
        if not self._ollama_cmd:
            return []
        env = os.environ.copy()
        if self.config.host:
            env["OLLAMA_HOST"] = self.config.host
        try:
            result = subprocess.run(
                ["ollama", "list", "--format", "json"],
                capture_output=True,
                text=True,
                env=env,
                timeout=5.0,
                check=False,
            )
        except Exception:
            return []
        if result.returncode != 0:
            return []
        try:
            payload = json.loads(result.stdout or "[]")
        except json.JSONDecodeError:
            return []
        models: List[str] = []
        if isinstance(payload, list):
            for entry in payload:
                if isinstance(entry, dict):
                    name = str(entry.get("name") or "").strip()
                    if name:
                        models.append(name)
        return models
