# Copy this file to `.env` and adjust values as needed.

# ──────────────────────────────────────
# LNP Chat – 로컬 LLM 연동 (llama.cpp / Ollama 등)
# ──────────────────────────────────────
LNPCHAT_LLM_BACKEND=local_llamacpp
LNPCHAT_LLM_MODEL=models/gguf/gemma-3-4b-it-Q4_K_M.gguf
# macOS(Apple Silicon)에서 Metal GPU 사용 시 -1(전체 offload) 또는 8~24 정도로 시작 추천.
LNPCHAT_LLAMACPP_GPU_LAYERS=-1
# llama-cpp-python을 subprocess로 실행(세그폴트/크래시 방지, 권장)
LNPCHAT_LLAMACPP_SUBPROCESS=1
# llama-cpp-python이 실패하면 llama-cli(Metal→CPU)로 자동 폴백(권장)
LNPCHAT_LLAMACPP_FALLBACK_CLI=1
# 강제로 llama-cli를 쓰려면:
# LNPCHAT_LLM_BACKEND=local_llama_cli
# 필요 시 경로 지정:
# LNPCHAT_LLAMA_CLI_PATH=models/llama.cpp/build_metal/bin/llama-cli
# Metal에서 mmap 문제가 있으면 1로:
# LNPCHAT_LLAMA_CLI_NO_MMAP=1
# Ollama 등 원격일 때만 지정. 기본은 비움.
LNPCHAT_LLM_HOST=
LNPCHAT_LLM_TIMEOUT=30       # 로컬/원격 LLM 응답 대기 시간(초)
LNPCHAT_AUTO_SEARCH=0          # /doc, /search, /audio 등 명령일 때만 검색/RAG 실행
LNPCHAT_RERANK=1
LNPCHAT_RERANK_MODEL=BAAI/bge-reranker-large
LNPCHAT_RERANK_DEPTH=80
LNPCHAT_MEMORY_TURNS=40

# ──────────────────────────────────────
# infopilot 런타임 로깅
# ──────────────────────────────────────
INFOPILOT_LOG_LEVEL=INFO
INFOPILOT_RUNTIME_LOG_DIR=artifacts/logs/runtime

# ──────────────────────────────────────
# 회의 비서 기본 경로
# ──────────────────────────────────────
# 회의 결과가 저장될 기본 폴더
MEETING_OUTPUT_DIR=data/ami_outputs
# 분석·대시보드 산출물 경로
MEETING_ANALYTICS_DIR=data/analytics

# ──────────────────────────────────────
# 회의 비서 – STT(Whisper) 설정
# ──────────────────────────────────────
MEETING_STT_BACKEND=faster-whisper
MEETING_STT_MODEL=small
# GPU를 사용할 경우 예: cuda, cuda:0
MEETING_STT_DEVICE=
# int8 / float16 등 whisper 컴퓨트 타입
MEETING_STT_COMPUTE=int8
# 사용자 지정 모델 캐시 경로(선택)
MEETING_STT_MODEL_DIR=

# ──────────────────────────────────────
# 회의 비서 – 요약기 설정
# ──────────────────────────────────────
# 기본 한국어 요약 모델(로컬 llama.cpp, Gemma3 4B GGUF)
MEETING_SUMMARY_BACKEND=llama
MEETING_SUMMARY_LLAMA_MODEL=models/gguf/gemma-3-4b-it-Q4_K_M.gguf
MEETING_SUMMARY_LLAMA_N_CTX=4096
MEETING_SUMMARY_LLAMA_THREADS=0
# macOS(Apple Silicon)에서 Metal GPU 사용 시 -1(전체 offload) 또는 8~24 정도로 시작 추천.
MEETING_SUMMARY_LLAMA_GPU_LAYERS=0
MEETING_SUMMARY_LLAMA_MAX_NEW_TOKENS=256
# macOS에서 안정성을 위해 llama-cpp-python을 subprocess로 실행(권장).
MEETING_LLAMA_CPP_SUBPROCESS=1
# llama 백엔드 우선순위:
#   auto = (1) llama_cpp direct(subprocess) → (2) llama-cli Metal → (3) llama-cli CPU
MEETING_LLAMA_MODE=auto
# 필요 시 llama-cli 경로를 강제 지정(예: models/llama.cpp/build_metal/bin/llama-cli)
# MEETING_LLAMA_CLI_PATH=
# Metal에서 mmap 문제가 있으면 1로 고정(기본: Metal일 때만 no-mmap)
# MEETING_LLAMA_CLI_NO_MMAP=
# in-process llama.cpp를 강제로 쓰고 싶을 때만(비권장; 세그폴트 가능) 1로 설정
# MEETING_ALLOW_LLAMA_CPP=1

# 영어 요약 모델(필요 시 교체)
MEETING_SUMMARY_EN_MODEL=facebook/bart-large-cnn

# Hugging Face/BART 등을 쓰고 싶다면 아래처럼 백엔드/모델을 바꿔주세요.
# MEETING_SUMMARY_MODEL=gogamza/kobart-base-v2
# MEETING_SUMMARY_BACKEND=kobart

# Ollama 사용 시 모델/호스트 지정
MEETING_SUMMARY_OLLAMA_MODEL=llama3
MEETING_SUMMARY_OLLAMA_HOST=

# BitNet 사용 시 모델/파라미터
MEETING_SUMMARY_BITNET_MODEL=bitnet/b1.58-instruct
MEETING_SUMMARY_BITNET_MAXLEN=220
MEETING_SUMMARY_BITNET_MINLEN=60

# 요약 길이 파라미터(선택)
MEETING_SUMMARY_MAXLEN=128
MEETING_SUMMARY_MINLEN=32
MEETING_SUMMARY_MAX_NEW_TOKENS=128
MEETING_SUMMARY_NUM_BEAMS=4
MEETING_SUMMARY_MODEL_MAX_CHARS=1024
MEETING_SUMMARY_CHUNK_CHARS=1800

# 기타 옵션
MEETING_SAVE_TRANSCRIPT=0
MEETING_MASK_PII=0
MEETING_STT_CHUNK_SECONDS=0

# ──────────────────────────────────────
# 회의 비서 – 확장 기능(선택)
# ──────────────────────────────────────
MEETING_RAG_ENABLED=0
MEETING_RAG_STORE=
MEETING_VECTOR_INDEX=

MEETING_INTEGRATIONS_PROVIDER=
MEETING_INTEGRATIONS_OUT=artifacts/integrations
MEETING_AUDIT_LOG=

MEETING_ONDEVICE_MODEL_PATH=
MEETING_ONDEVICE_MODEL_NAME=
MEETING_ONDEVICE_DEVICE=cpu
MEETING_ONDEVICE_MMAP=1

MEETING_RETRAIN_OUTPUT_DIR=artifacts/retraining
MEETING_BASE_MODEL=
MEETING_FEEDBACK_INBOX=

# ──────────────────────────────────────
# 기타 옵션
# ──────────────────────────────────────
# 추가 CustomTkinter UI 설정, 정책 경로 등을 여기에 정의할 수 있습니다.
