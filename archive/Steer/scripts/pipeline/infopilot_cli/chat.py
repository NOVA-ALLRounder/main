from __future__ import annotations

import json
import os
import sys
from pathlib import Path
from types import SimpleNamespace
from typing import Any, Callable, Dict, Iterator, List, Optional

from core.agents.document import DocumentAgent, DocumentAgentConfig
from core.agents.meeting import MeetingAgent
from core.agents.photo import PhotoAgent, PhotoAgentConfig
from core.data_pipeline.pipeline import run_step2
from core.policy.engine import PolicyEngine
from core.errors import PolicyViolationError

from core.conversation.orchestrator import AssistantOrchestrator
from core.conversation.llm_client import create_llm_client

from .history import load_agent_history, remember_agent_history
from .policy import enforce_cache_limit, load_policy_engine
from .scan_rows import load_scan_rows, resolve_scan_csv
from .train_config import default_train_config
from core.config.llm_defaults import DEFAULT_LLM_MODEL


def ensure_chat_artifacts(
    *,
    scan_csv: Path,
    corpus: Path,
    model: Path,
    translate: bool,
    auto_train: bool,
    policy_engine: Optional[PolicyEngine],
    policy_agent: str,
) -> bool:
    """Ensure chat artifacts exist and are up to date. Returns True if training ran."""

    def mtime(path: Path) -> float:
        try:
            return path.stat().st_mtime
        except OSError:
            return 0.0

    resolved_scan: Optional[Path] = None
    if scan_csv:
        try:
            resolved_scan = resolve_scan_csv(scan_csv)
        except FileNotFoundError:
            resolved_scan = None

    artifacts_exist = corpus.exists() and model.exists()
    needs_train = not artifacts_exist

    if not needs_train and resolved_scan:
        scan_mtime = mtime(resolved_scan)
        artifacts_mtime = min(mtime(corpus), mtime(model))
        if scan_mtime > artifacts_mtime:
            needs_train = True

    if not needs_train:
        print("ğŸ”„ ì¸ë±ìŠ¤ ìµœì‹ ì„± í™•ì¸ ì™„ë£Œ.")
        return False

    if resolved_scan is None:
        msg = (
            "âš ï¸ í•™ìŠµ ì‚°ì¶œë¬¼ì´ ì—†ê±°ë‚˜ ì˜¤ë˜ë˜ì—ˆì§€ë§Œ ì‚¬ìš©í•  ìŠ¤ìº” CSVë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤."
            " `--scan_csv` ê²½ë¡œë¥¼ í™•ì¸í•˜ê±°ë‚˜ 'scan' ëª…ë ¹ì„ ë‹¤ì‹œ ì‹¤í–‰í•´ì£¼ì„¸ìš”."
        )
        raise FileNotFoundError(msg)

    if not auto_train:
        raise RuntimeError(
            "í•™ìŠµ ì‚°ì¶œë¬¼ì´ ìµœì‹ ì´ ì•„ë‹™ë‹ˆë‹¤. 'python infopilot.py train --scan_csv "
            f"{resolved_scan}'ë¥¼ ì‹¤í–‰í•œ ë’¤ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”."
        )

    print("âš ï¸ ìŠ¤ìº” ê²°ê³¼ê°€ ëª¨ë¸ë³´ë‹¤ ìµœì‹ ì…ë‹ˆë‹¤. ìë™ìœ¼ë¡œ train ë‹¨ê³„ë¥¼ ì‹¤í–‰í•©ë‹ˆë‹¤.")
    rows = list(
        load_scan_rows(
            resolved_scan,
            policy_engine=policy_engine,
            include_manual=False,
            agent=policy_agent,
        )
    )
    if not rows:
        raise ValueError("ìë™ í•™ìŠµì„ ìœ„í•œ ìœ íš¨í•œ í–‰ì´ ì—†ìŠµë‹ˆë‹¤. ìŠ¤ìº” ê²°ê³¼ë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”.")

    cfg = default_train_config()
    run_step2(
        rows,
        out_corpus=corpus,
        out_model=model,
        cfg=cfg,
        use_tqdm=True,
        translate=translate,
    )
    print("âœ… ìë™ í•™ìŠµ ì™„ë£Œ")
    return True


def cmd_chat(
    args,
    *,
    default_policy_path: Path,
    policy_agent: str,
) -> None:
    policy_arg = getattr(args, "policy", None)
    policy_normalized = (policy_arg or "").strip().lower()
    policy_required = policy_normalized != "none"
    policy_engine = load_policy_engine(
        policy_arg,
        default_policy_path=default_policy_path,
        fail_if_missing=policy_required,
        stage="chat",
    )

    def _env_or_arg(name: str, default: Optional[str] = None) -> Optional[str]:
        value = getattr(args, name, None)
        if value:
            value = str(value).strip()
            if value:
                return value
        env_name = f"LNPCHAT_{name.upper()}"
        env_value = os.getenv(env_name)
        if env_value is None:
            return default
        env_value = env_value.strip()
        return env_value or default

    single_query = getattr(args, "query", None)
    json_mode = bool(getattr(args, "json", False))
    # if json_mode and not single_query:
    #     raise SystemExit("--json ì˜µì…˜ì€ --queryì™€ í•¨ê»˜ ì‚¬ìš©í•´ì•¼ í•©ë‹ˆë‹¤.")

    llm_backend = _env_or_arg("llm_backend", default="none")
    llm_model = _env_or_arg("llm_model", default=DEFAULT_LLM_MODEL)
    llm_host = _env_or_arg("llm_host", default="")

    llm_client = None
    if llm_backend and llm_backend.lower() != "none":
        try:
            llm_client = create_llm_client(
                backend=llm_backend,
                model=llm_model,
                host=llm_host,
                options={},
            )
            print(f"â„¹ï¸ ë¡œì»¬ LLM ì—°ê²°: {llm_backend}/{llm_model}")
        except Exception as e:
            print(f"âš ï¸ ë¡œì»¬ LLM ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")

    auto_trained = ensure_chat_artifacts(
        scan_csv=Path(args.scan_csv),
        corpus=Path(args.corpus),
        model=Path(args.model),
        translate=args.translate,
        auto_train=args.auto_train,
        policy_engine=policy_engine,
        policy_agent=policy_agent,
    )

    document_agent = DocumentAgent(
        DocumentAgentConfig(
            model_path=Path(args.model),
            corpus_path=Path(args.corpus),
            cache_dir=Path(args.cache),
            topk=args.topk,
            translate=args.translate,
            rerank=args.rerank,
            rerank_model=args.rerank_model,
            rerank_depth=args.rerank_depth,
            rerank_batch_size=args.rerank_batch_size,
            rerank_device=args.rerank_device or None,
            rerank_min_score=args.rerank_min_score,
            lexical_weight=args.lexical_weight,
            show_translation=args.show_translation,
            translation_lang=args.translation_lang,
            min_similarity=args.min_similarity,
            strict_search=bool(getattr(args, "strict", False)),
            llm_backend=llm_backend,
            llm_model=llm_model,
            llm_host=llm_host,
            llm_options={},
            policy_engine=policy_engine if policy_engine and policy_engine.has_policies else policy_engine,
            policy_scope=(getattr(args, "scope", "auto") or "auto").lower(),
            policy_agent=policy_agent,
            rebuild_index=auto_trained,
        )
    )
    agents = [document_agent]
    # Allow /meeting, /photo commands even in --json mode (non-interactive),
    # but keep heavy agent dependencies (llama.cpp / transformers) out of the default path.
    agents.extend(
        [
            MeetingAgent(policy_engine=policy_engine),
            PhotoAgent(PhotoAgentConfig(policy_engine=policy_engine, policy_tag=str(getattr(args, "policy", "") or ""))),
        ]
    )

    # Use LLM for intelligent routing (re-enabled for better intent recognition)
    orchestrator = AssistantOrchestrator(agents, llm_client=llm_client)

    # -------------------------------------------------------------------------
    # Sprint 2: Performance Optimization (Pre-warming)
    # -------------------------------------------------------------------------
    def _warmup_models(agent_list: List[Any]) -> None:
        """Background thread to pre-load heavy models (Embedding, Whisper, etc)."""
        import threading
        # print("ğŸ”¥ Warming up models in background...", file=sys.stderr)
        for agent in agent_list:
            if hasattr(agent, "prepare"):
                try:
                    # DocumentAgent: loads embedding model
                    # MeetingAgent: ensures policy engine / STT backend check
                    agent.prepare()
                except Exception as e:
                    # Ignore warmup errors to avoid crashing main thread
                    pass
        # print("âœ… Model warmup completed.", file=sys.stderr)

    import threading
    warmup_thread = threading.Thread(target=_warmup_models, args=(agents,), daemon=True)
    warmup_thread.start()
    # -------------------------------------------------------------------------

    def _print_response(resp) -> None:
        prefix = f"[{resp.agent}] " if getattr(resp, "agent", None) else ""
        print(prefix + getattr(resp, "message", ""))
        suggestions = getattr(resp, "suggestions", None) or []
        if suggestions:
            print("\nğŸ’¡ ì´ëŸ° ì§ˆë¬¸ì€ ì–´ë– ì„¸ìš”?")
            for suggestion in suggestions:
                print(f"   - {suggestion}")

    def _cli_progress_handler(agent_label: str, json_mode: bool = False) -> Callable[[Dict[str, Any]], None]:
        def _handler(event: Dict[str, Any]) -> None:
            stage = event.get("stage")
            status = event.get("status")
            
            # Special case for real-time STT streaming
            if status == "streaming":
                chunk = event.get("chunk")
                if json_mode and chunk:
                    print(json.dumps({"status": "streaming", "content": chunk}, ensure_ascii=False), flush=True)
                return

            prefix = f"[{agent_label}]"
            if json_mode:
                # Optional: emit stage updates as structured JSON logs if desired
                # For now, we only care about 'streaming' events for the UI visualizer
                return

            if status == "running":
                print(f"{prefix} â–¶ {stage} ì‹œì‘")
            elif status == "completed":
                print(f"{prefix} âœ… {stage} ì™„ë£Œ")
            elif status == "failed":
                error = event.get("error")
                print(f"{prefix} âŒ {stage} ì‹¤íŒ¨: {error}")
            elif status == "cancelled":
                print(f"{prefix} â›” {stage} ì·¨ì†Œ")
        return _handler

    def _prompt_follow_up(reason: Optional[str], message: str) -> Optional[Dict[str, object]]:
        print(message)
        history = load_agent_history()
        if reason == "needs_audio":
            recent = history.get("meeting_audio", [])
            if recent:
                print("\nğŸ“ ìµœê·¼ ì‚¬ìš©í•œ ì˜¤ë””ì˜¤ íŒŒì¼:")
                for idx, item in enumerate(recent, start=1):
                    print(f"  {idx}. {item}")
            prompt = "íšŒì˜ ìš”ì•½ì„ ì‹¤í–‰í•˜ë ¤ë©´ ì˜¤ë””ì˜¤ íŒŒì¼ ì „ì²´ ê²½ë¡œë¥¼ ì…ë ¥í•˜ê±°ë‚˜ ë²ˆí˜¸ë¥¼ ì„ íƒí•˜ì„¸ìš”> "
            raw = input(prompt).strip()
            if not raw:
                print("âš ï¸ ê²½ë¡œë¥¼ ì…ë ¥í•˜ì§€ ì•Šì•„ ìš”ì²­ì„ ì·¨ì†Œí–ˆìŠµë‹ˆë‹¤.")
                return None
            if raw.isdigit():
                index = int(raw) - 1
                if 0 <= index < len(recent):
                    audio_path = recent[index]
                else:
                    print("âš ï¸ ë²ˆí˜¸ê°€ ìœ íš¨í•˜ì§€ ì•Šì•„ ìš”ì²­ì„ ì·¨ì†Œí–ˆìŠµë‹ˆë‹¤.")
                    return None
            else:
                audio_path = raw
            remember_agent_history("meeting_audio", [audio_path])
            return {"audio_path": audio_path, "enable_resume": True}
        if reason == "needs_roots":
            recent = history.get("photo_roots", [])
            if recent:
                print("\nğŸ“¸ ìµœê·¼ ì‚¬ìš©í•œ ì‚¬ì§„ í´ë”:")
                for idx, item in enumerate(recent, start=1):
                    print(f"  {idx}. {item}")
            prompt = "ì‚¬ì§„ í´ë” ê²½ë¡œë¥¼ ì…ë ¥í•˜ê±°ë‚˜ ë²ˆí˜¸(ì—¬ëŸ¬ ê°œëŠ” ì½¤ë§ˆ)ë¡œ ì„ íƒí•˜ì„¸ìš”> "
            raw = input(prompt).strip()
            if not raw:
                print("âš ï¸ ê²½ë¡œë¥¼ ì…ë ¥í•˜ì§€ ì•Šì•„ ìš”ì²­ì„ ì·¨ì†Œí–ˆìŠµë‹ˆë‹¤.")
                return None
            roots: List[str] = []
            for token in [part.strip() for part in raw.split(",") if part.strip()]:
                if token.isdigit():
                    index = int(token) - 1
                    if 0 <= index < len(recent):
                        roots.append(recent[index])
                    else:
                        print(f"âš ï¸ ë²ˆí˜¸ {token}ê°€ ìœ íš¨í•˜ì§€ ì•Šì•„ ë¬´ì‹œí•©ë‹ˆë‹¤.")
                else:
                    roots.append(token)
            if not roots:
                print("âš ï¸ ìœ íš¨í•œ ê²½ë¡œê°€ ì—†ì–´ ìš”ì²­ì„ ì·¨ì†Œí–ˆìŠµë‹ˆë‹¤.")
                return None
            remember_agent_history("photo_roots", roots)
            return {"roots": roots}
        extra = input("ì¶”ê°€ ì •ë³´ë¥¼ ì…ë ¥í•˜ì„¸ìš”> ").strip()
        if not extra:
            print("âš ï¸ ì¶”ê°€ ì •ë³´ë¥¼ ì…ë ¥í•˜ì§€ ì•Šì•„ ìš”ì²­ì„ ì·¨ì†Œí–ˆìŠµë‹ˆë‹¤.")
            return None
        return {"details": extra}

    def _resolve_follow_up(original_query: str, initial_response, json_mode: bool = False) -> Any:
        response = initial_response
        while getattr(response, "agent", None) == "follow_up":
            follow_context = _prompt_follow_up(getattr(response, "reason", None), getattr(response, "message", ""))
            if not follow_context:
                break
            if getattr(response, "reason", None) == "needs_audio":
                follow_context.setdefault("__progress_callback", _cli_progress_handler("íšŒì˜ ë¹„ì„œ", json_mode=json_mode))
            elif getattr(response, "reason", None) == "needs_roots":
                follow_context.setdefault("__progress_callback", _cli_progress_handler("ì‚¬ì§„ ë¹„ì„œ", json_mode=json_mode))
            response = orchestrator.handle(original_query, follow_context)
        return response

    enforce_cache_limit(
        Path(args.cache),
        policy_engine,
        hard_limit=getattr(args, "cache_hard_limit", False),
        clean_on_limit=getattr(args, "cache_clean_on_limit", False),
    )

    base_context = {"policy_engine": policy_engine} if policy_engine and policy_engine.has_policies else {"policy_engine": policy_engine}

    if single_query:
        response = orchestrator.handle(single_query, base_context)
        if getattr(response, "agent", None) == "follow_up" and not json_mode:
            response = _resolve_follow_up(single_query, response, json_mode=json_mode)
        if json_mode:
            print(json.dumps(_build_chat_response_payload(single_query, response, args.topk), ensure_ascii=False))
        else:
            _print_response(response)
        return

    if not json_mode:
        print(
            "\nğŸ’¬ InfoPilot Chat ëª¨ë“œì…ë‹ˆë‹¤. ììœ ë¡­ê²Œ ëŒ€í™”í•˜ê³ , ë¬¸ì„œ ê²€ìƒ‰ì´ í•„ìš”í•˜ë©´ '/search ì§ˆë¬¸'ì²˜ëŸ¼ ì…ë ¥í•´ ë³´ì„¸ìš”. "
            "(ì¢…ë£Œí•˜ë ¤ë©´ 'exit' ë˜ëŠ” 'ì¢…ë£Œ' ì…ë ¥)"
        )
        print("   ëª…ë ¹ì–´: /search <ì§ˆë¬¸>, /meeting, /photo")

    while True:
        try:
            if not sys.stdin.isatty():
                query = sys.stdin.read().strip()
                if not query:  # EOF
                    break
            else:
                query = input("ì§ˆë¬¸> ").strip()
        except (EOFError, KeyboardInterrupt):
            if not json_mode:
                print("\nğŸ‘‹ ì¢…ë£Œí•©ë‹ˆë‹¤.")
            break

        if not query:
            continue
        if query.lower() in {"exit", "quit", "ì¢…ë£Œ"}:
            if not json_mode:
                print("ğŸ‘‹ ì¢…ë£Œí•©ë‹ˆë‹¤.")
            break

        response = orchestrator.handle(query, base_context)
        
        # Follow-up resolution
        if getattr(response, "agent", None) == "follow_up" and not json_mode:
            response = _resolve_follow_up(query, response, json_mode=json_mode)

        if json_mode:
            print(json.dumps(_build_chat_response_payload(query, response, args.topk), ensure_ascii=False))
            sys.stdout.flush()
        else:
            _print_response(response)
            print("-" * 80)


def _build_chat_response_payload(query: str, response: Any, topk: int) -> Dict[str, Any]:
    metadata = response.metadata if isinstance(response.metadata, dict) else {}
    payload = {
        "query": query,
        "answer": response.message,
        "agent": response.agent,
        "reason": response.reason,
        "metadata": metadata,
        "suggestions": response.suggestions or [],
        "results": [],
    }
    if response.agent == "follow_up":
        metadata["follow_up"] = response.reason
    hits = metadata.get("hits", []) or []
    for hit in hits[:topk]:
        payload["results"].append(
            {
                "title": Path(str(hit.get("path") or "")).name,
                "path": hit.get("path"),
                "ext": hit.get("ext"),
                "score": hit.get("similarity", hit.get("vector_similarity")),
                "vector_score": hit.get("vector_similarity"),
                "lexical_score": hit.get("lexical_score"),
                "match_reasons": hit.get("match_reasons") or [],
                "preview": hit.get("preview"),
                "translation": hit.get("translation"),
            }
        )
    return payload


__all__ = ["cmd_chat", "ensure_chat_artifacts"]
